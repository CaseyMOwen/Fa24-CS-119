{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS119 Big Data\n",
    "\n",
    "Spring 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Functional Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Create Arithmetic Functional Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def add(*args):\n",
    "    return reduce(lambda x, y: x + y, args, 0)\n",
    "\n",
    "def sub(*args):\n",
    "    return reduce(lambda x, y: x - y, args)\n",
    "\n",
    "def ra_sub(*args):\n",
    "    if len(args) == 1:\n",
    "        return args[0]\n",
    "    else:\n",
    "        # Unpack the tuple so each element is one argument\n",
    "        return args[0] - ra_sub(*args[1:]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the outputs against known answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "CORRECT\n",
      "2\n",
      "CORRECT\n",
      "6\n",
      "CORRECT\n"
     ]
    }
   ],
   "source": [
    "add_result = add(1, 2, 3)\n",
    "print(add_result)\n",
    "if add_result == 6:\n",
    "    print(\"CORRECT\")\n",
    "else:\n",
    "    print(\"INCORRECT\")\n",
    "\n",
    "sub_result = sub(5, 1, 2)\n",
    "print(sub_result)\n",
    "if sub_result == 2:\n",
    "    print(\"CORRECT\")\n",
    "else:\n",
    "    print(\"INCORRECT\")\n",
    "\n",
    "ra_sub_result = ra_sub(5, 1, 2)\n",
    "print(ra_sub_result)\n",
    "if ra_sub_result == 6:\n",
    "    print(\"CORRECT\")\n",
    "else:\n",
    "    print(\"INCORRECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Create Zip Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_to_zipped(zipped, seq):\n",
    "    # Concatenates the elements of the existing zipped, and the sequence, for each element\n",
    "    return list(map(lambda zipped, seq: [*zipped, seq], zipped, seq))\n",
    "\n",
    "def my_zip(*args):\n",
    "    # Starts with a series of empty lists, one for each element of a given sequence (all must be the same length), and appends to it, one sequence at a time\n",
    "    return reduce(add_to_zipped, args, [[]]*len(args[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the outputs against known answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4], [2, 5], [3, 6]]\n",
      "CORRECT\n",
      "[[1, 4, 7], [2, 5, 8], [3, 6, 9]]\n",
      "CORRECT\n"
     ]
    }
   ],
   "source": [
    "zip_result_1 = my_zip([1,2,3],[4,5,6])\n",
    "print(zip_result_1)\n",
    "if zip_result_1 == [[1, 4], [2, 5], [3, 6]]:\n",
    "    print(\"CORRECT\")\n",
    "else:\n",
    "    print(\"INCORRECT\")\n",
    "\n",
    "zip_result_2 = my_zip([1,2,3],[4,5,6],[7,8,9])\n",
    "print(zip_result_2)\n",
    "if zip_result_2 == [[1, 4, 7], [2, 5, 8], [3, 6, 9]]:\n",
    "    print(\"CORRECT\")\n",
    "else:\n",
    "    print(\"INCORRECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Create Zipwith Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipwith(f, *args):\n",
    "    return list(map(f, *args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the outputs against known answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7, 9]\n",
      "CORRECT\n",
      "[6, 8, 10]\n",
      "CORRECT\n"
     ]
    }
   ],
   "source": [
    "zipwith_result_1 = zipwith(add, [1, 2, 3], [4, 5, 6])\n",
    "print(zipwith_result_1)\n",
    "if zipwith_result_1 == [5, 7, 9]:\n",
    "    print(\"CORRECT\")\n",
    "else:\n",
    "    print(\"INCORRECT\")\n",
    "\n",
    "zipwith_result_2 = zipwith(add, [1, 2, 3], [4, 5, 6], [1, 1, 1])\n",
    "print(zipwith_result_2)\n",
    "if zipwith_result_2 == [6, 8, 10]:\n",
    "    print(\"CORRECT\")\n",
    "else:\n",
    "    print(\"INCORRECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Create Flatten Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def concat_ints(x, y):\n",
    "    '''\n",
    "    Concatenates two values, x and y, that may either be ints or lists of ints, into a single list of ints\n",
    "    '''\n",
    "    if isinstance(x, int):\n",
    "        if isinstance(y, int):\n",
    "            return [x, y]\n",
    "        else:\n",
    "            return [x, *y]\n",
    "    else:\n",
    "        if isinstance(y, int):\n",
    "            return [*x, y]\n",
    "        else:\n",
    "            return [*x, *y]\n",
    "\n",
    "def flatten(tree:list):\n",
    "    # Recursion base case is if all elements of tree are ints rather than lists\n",
    "    if reduce(lambda prev, x: prev and isinstance(x, int), tree, True):\n",
    "        return tree\n",
    "    else:\n",
    "        return flatten(reduce(concat_ints, tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the outputs against known answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "CORRECT\n"
     ]
    }
   ],
   "source": [
    "flatten_result = flatten([1, [2, [3, 4], [5, 6], 7], 8, [9, 10]])\n",
    "print(flatten_result)\n",
    "if flatten_result == [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    print(\"CORRECT\")\n",
    "else:\n",
    "    print(\"INCORRECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Create Groupby Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "\n",
    "def add_key_to_dict(d: dict, tup: tuple):\n",
    "    key, val = tup\n",
    "    # Need to deepcopy since dicts are mutable - not editing the existing d\n",
    "    new_dict = deepcopy(d)\n",
    "    if key in new_dict:\n",
    "        old_val = d[key]\n",
    "    else:\n",
    "        old_val = []\n",
    "    # This combines dicts into new, updating keys from d with new key\n",
    "    return new_dict | {key: old_val + [val]}\n",
    "\n",
    "def group_by(func, seq):\n",
    "    func_outputs =list(map(func, seq)) \n",
    "    zipped = my_zip(func_outputs, seq)\n",
    "    return reduce(add_key_to_dict, zipped, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the outputs against known answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: ['hi', 'me'], 3: ['dog', 'bad'], 4: ['good']}\n",
      "CORRECT\n"
     ]
    }
   ],
   "source": [
    "grouby_result = group_by(len, [\"hi\", \"dog\", \"me\", \"bad\", \"good\"])\n",
    "print(grouby_result)\n",
    "if grouby_result == {2: [\"hi\", \"me\"], 3: [\"dog\", \"bad\"], 4: [\"good\"]}:\n",
    "    print(\"CORRECT\")\n",
    "else:\n",
    "    print(\"INCORRECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Confirming Hadoop Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Aquire the Cluster\n",
    "\n",
    "![P2Q1](img/P2Q1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Load the data into the master\n",
    "\n",
    "Make quiz4 directory on hdfs:\n",
    "\n",
    "**hadoop fs -mkdir /quiz4**\n",
    "\n",
    "Get assignment file via curl and put it in the new folder as access.log:\n",
    "\n",
    "**curl -sS https://raw.githubusercontent.com/singhj/big-data-repo/refs/heads/main/datasets/access.log | hadoop fs -put - /quiz4/access.log**\n",
    "\n",
    "View the outputs:\n",
    "\n",
    "**hadoop fs -cat /quiz4/access.log | head**\n",
    "\n",
    "output is top 10 rows of raw file, as expected\n",
    "\n",
    "![P2Q2](img/P2Q2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Run Wordcount on five-books\n",
    "\n",
    "Run hadoop jar command to create mapreduce job:\n",
    "\n",
    "**hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /five-books /books-count**\n",
    "\n",
    "Get results of job once complete:\n",
    "\n",
    "**hadoop fs -get /books-count**\n",
    "\n",
    "View results:\n",
    "\n",
    "![P2Q3_1](img/P2Q3_1.png)\n",
    "\n",
    "![P2Q3_2](img/P2Q3_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Run Wordcount using mapper_noll and aggregate\n",
    "\n",
    "Run mapred streaming command to create mapreduce job:\n",
    "\n",
    "**mapred streaming -file ~/big-data-repo/hadoop/mapper_noll.py -mapper mapper_noll.py -input /five-books -reducer aggregate -output /books-stream-count**\n",
    "\n",
    "Get results of job onto master once complete:\n",
    "\n",
    "**hadoop fs -get /books-stream-count**\n",
    "\n",
    "View results:\n",
    "\n",
    "![P2Q4](img/P2Q4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Run wordcount using mapper_noll and reducer_noll\n",
    "\n",
    "Run mapred streaming command to create mapreduce job:\n",
    "\n",
    "**mapred streaming -file ~/big-data-repo/hadoop/mapper_noll.py -file ~/big-data-repo/hadoop/reducer_noll.py -mapper mapper_noll.py -reducer reducer_noll.py -input /five-books -output /books-my-own-counts**\n",
    "\n",
    "Get results of job onto master once complete:\n",
    "\n",
    "**hadoop fs -get /books-my-own-counts**\n",
    "\n",
    "View results, which are word counts formatted accoring to custom reducer, as expected\n",
    "\n",
    "![P2Q5](img/P2Q5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Analyzing Server Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Get the percentage of each request type (GET, PUT, POST, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Command and Results:\n",
    "\n",
    "Put files from master onto hdfs after uploading\n",
    "\n",
    "**hadoop fs -put quiz4/Part3Question1_reducer.py quiz4/Part3Question1_mapper.py /quiz4/**\n",
    "\n",
    "MapReduce Job\n",
    "\n",
    "**mapred streaming -file ~/quiz4/Part3Question1_mapper.py -file ~/quiz4/Part3Question1_reducer.py -mapper Part3Question1_mapper.py -reducer Part3Question1_reducer.py -input /quiz4/access.log -output /quiz4/Part3Question1**\n",
    "\n",
    "Get Results from hdfs to master:\n",
    "\n",
    "**hadoop fs -get /quiz4/Part3Question1 \\\\quiz4**\n",
    "\n",
    "Post-Processing to turn counts into Percentages.\n",
    "Bash command that loops through concatenated reducer results twice - first to sum up total occurences, second time to divide by the sum\n",
    "\n",
    "**awk 'NR==FNR{sum+= $2; next}{$2/=sum; print $0}' <(cat quiz4/Part3Question1/\\*) <(cat quiz4/Part3Question1/\\*) > quiz4/Part3Question1Results.txt**\n",
    "\n",
    "Results:\n",
    "\n",
    "![P3Q1](img/P3Q1.png)\n",
    "\n",
    "I did this as one mapreduce job to simply create the counts of each unique term, then a simple bash command to transform the counts into percentages. This could not have been done as one single job to calculate the percents with multiple reduce tasks, since we can't know the overall count of all keys until all reducers are finished, and therefore can't calculate the percentage within any given reducer. So I framed the problem as a word counting, and a postprocessing command. This method is generalizeable to extremely large data since there were only 3 possible requests, so the bash computation to convert to percentages was trivial and did not necessitate another mapreduce job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part3Question1_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys, shlex\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    line = sys.stdin.readline()\n",
    "    try:\n",
    "        while line:\n",
    "            # Using shlex to split since it more naturally splits on the file - it uses spaces for delimeters, with quoted fields that sometimes contain spaces\n",
    "            linelist = shlex.split(line)\n",
    "            # Only consider input if we get full row exactly - may be slightly different if delimiter changes\n",
    "            if len(linelist) == 11:\n",
    "                # The command with the parameter is the 6th column\n",
    "                request = linelist[5]\n",
    "                # The request type itself is space seperated from the rest of the command\n",
    "                request_type = request.split(' ')[0]\n",
    "                print(\"RequestType:\" + request_type.upper() + \"\\t\" + \"1\")\n",
    "            line = sys.stdin.readline()\n",
    "    except EOFError as error:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part3Question1_reducer.py (same as reducer_noll.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"reducer.py\"\"\"\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print ('%s\\t%s' % (current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print ('%s\\t%s' % (current_word, current_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Get the percentage of each response type (100-199, 200-299, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar process to Question 1 - the main difference is the mapper file\n",
    "\n",
    "#### Command and Results:\n",
    "\n",
    "Put files from master onto hdfs\n",
    "\n",
    "**hadoop fs -put quiz4/Part3Question2_reducer.py quiz4/Part3Question2_mapper.py /quiz4/**\n",
    "\n",
    "MapReduce Job\n",
    "\n",
    "**mapred streaming -file ~/quiz4/Part3Question2_mapper.py -file ~/quiz4/Part3Question2_reducer.py -mapper Part3Question2_mapper.py -reducer Part3Question2_reducer.py -input /quiz4/access.log -output /quiz4/Part3Question2**\n",
    "\n",
    "Get Results from hdfs to master:\n",
    "\n",
    "**hadoop fs -get /quiz4/Part3Question2 \\\\quiz4**\n",
    "\n",
    "Post-Processing to turn counts into Percentages, same command as before with different files\n",
    "\n",
    "**awk 'NR==FNR{sum+= $2; next}{$2/=sum; print $0}' <(cat quiz4/Part3Question2/\\*) <(cat quiz4/Part3Question2/\\*) > quiz4/Part3Question2Results.txt**\n",
    "\n",
    "Results:\n",
    "\n",
    "![P3Q2](img/P3Q2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part3Question2_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys, shlex\n",
    "\n",
    "def main(argv):\n",
    "    line = sys.stdin.readline()\n",
    "    try:\n",
    "        while line:\n",
    "            linelist = shlex.split(line)\n",
    "            # Only consider input if we get full row exactly - may be slightly different if delimiter changes\n",
    "            if len(linelist) == 11:\n",
    "                response = linelist[6]\n",
    "                # Only need to consider leading digit of the response code for grouping\n",
    "                response_type = response[0] + \"00-\" + response[0] + \"99\"\n",
    "                print(\"ResponseType:\" + response_type + \"\\t\" + \"1\")\n",
    "            line = sys.stdin.readline()\n",
    "    except EOFError as error:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part3Question2_reducer.py (again, same as reducer_noll.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"reducer.py\"\"\"\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print ('%s\\t%s' % (current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print ('%s\\t%s' % (current_word, current_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Get the 5 IP addresses that return the most client errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar process to Question 1 and 2 - the main differences are the mapper file, and the command at the end \n",
    "\n",
    "#### Command and Results:\n",
    "\n",
    "Put files from master onto hdfs\n",
    "\n",
    "**hadoop fs -put quiz4/Part3Question3_reducer.py quiz4/Part3Question3_mapper.py /quiz4/**\n",
    "\n",
    "MapReduce Job\n",
    "\n",
    "**mapred streaming -file ~/quiz4/Part3Question3_mapper.py -file ~/quiz4/Part3Question3_reducer.py -mapper Part3Question3_mapper.py -reducer Part3Question3_reducer.py -input /quiz4/access.log -output /quiz4/Part3Question3**\n",
    "\n",
    "Get Results from hdfs to master:\n",
    "\n",
    "**hadoop fs -get /quiz4/Part3Question3 \\\\quiz4**\n",
    "\n",
    "Post-Processing to sort the counts, and take the top five\n",
    "\n",
    "**cat quiz4/Part3Question3/\\* | sort -r -n -k 2 | head -5 > quiz4/Part3Question3Results.txt**\n",
    "\n",
    "This command requires the -r, -n, and -k 2 flags to sort the results in descending order, numerically instead of alphabetically, and by the second column which is the count. \n",
    "\n",
    "Results:\n",
    "\n",
    "![P3Q3](img/P3Q3.png)\n",
    "\n",
    "This approach of using a post-processing command is somewhat less scalable than the question 1 and 2 approaches, since there may be a significant amount of unique IP addresses in extremely large data. The Mapreduce job did reduce the size of the file that needs to be processed from 78,252 lines, 12 columns down to 803 lines, 2 columns of the reducer output. If working with data that contains millions or billions of unique IP addresses, extra thought will be required. This could be a simple change, though - for example, you could pass the results into another mapreduce job that filtered the IP address counts by counts greater than some threshold (maybe 10), which would greatly reduce the number of rows and thus the sorting workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part3Question3_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys, shlex\n",
    "\n",
    "def main(argv):\n",
    "    line = sys.stdin.readline()\n",
    "    try:\n",
    "        while line:\n",
    "            linelist = shlex.split(line)\n",
    "            # Only consider input if we get full row exactly - may be slightly different if delimeter changes\n",
    "            if len(linelist) == 11:\n",
    "                response = linelist[6]\n",
    "                # Is a client error if the response code starts with a 4\n",
    "                if response[0] == '4':\n",
    "                    ip_address = linelist[0]\n",
    "                    print(\"IPAddress:\" + ip_address + \"\\t\" + \"1\")\n",
    "            line = sys.stdin.readline()\n",
    "    except EOFError as error:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part3Question3_reducer.py (again, same as reducer_noll.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"reducer.py\"\"\"\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print ('%s\\t%s' % (current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print ('%s\\t%s' % (current_word, current_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Presidential Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar process again as each Question in Part 3, with custom made mapper and reducer files\n",
    "\n",
    "#### Command and Results:\n",
    "\n",
    "Put files from master onto hdfs\n",
    "\n",
    "**hadoop fs -put quiz4/Part4_reducer.py quiz4/Part4_mapper.py /quiz4/**\n",
    "\n",
    "MapReduce Job\n",
    "\n",
    "**mapred streaming -file ~/quiz4/Part4_mapper.py -file ~/quiz4/Part4_reducer.py -mapper Part4_mapper.py -reducer Part4_reducer.py -input /quiz4/prez_speeches/\\* -output /quiz4/Part4**\n",
    "\n",
    "Get Results from hdfs to master:\n",
    "\n",
    "**hadoop fs -get /quiz4/Part4 \\\\quiz4**\n",
    "\n",
    "Post-Processing to sort the counts in descending order by the second column, which is valence\n",
    "\n",
    "**cat  quiz4/Part4/\\* | sort -r -n -k 2 > quiz4/Part4Results.txt**\n",
    "\n",
    "Results:\n",
    "\n",
    "![P4_1](img/P4_1.png)\n",
    "\n",
    "No pattern here between jumps out to me - about either predidential dispoisition or current events at the time of their presidency.\n",
    "\n",
    "![P4_2](img/P4_2.png)\n",
    "\n",
    "Map Wrote 3,300,913 total bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valence Function (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_stopwords(stopwords, words):\n",
    "    list_ = re.sub(r\"[^a-zA-Z0-9]\", \" \", words.lower()).split()\n",
    "    return [itm for itm in list_ if itm not in stopwords]\n",
    "\n",
    "def clean_text(stopwords, text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('[\\d\\n]', ' ', text)\n",
    "    return ' '.join(remove_stopwords(stopwords,  text))\n",
    "\n",
    "def calc_word_valence(word, afinn_dict):\n",
    "    if word in afinn_dict:\n",
    "        return int(afinn_dict[word])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def calc_valence(text, afinn_dict):\n",
    "    '''\n",
    "    Gets the valence of a line of cleaned text, returned as a list of valences at each word\n",
    "    '''\n",
    "    # At this point they will have been cleaned, so we assume a space seperator\n",
    "    word_valences = list(map(lambda word: calc_word_valence(word, afinn_dict), text.split(' ')))\n",
    "    return list(filter(lambda valence: valence is not None, word_valences))\n",
    "\n",
    "def valence(text):\n",
    "    '''\n",
    "    Gets the valence of a line of raw text\n",
    "    '''\n",
    "    # Using afinn_dict and stopwords as inputs so I don't have to load them anew for every line - just once at beginning of mapper\n",
    "    stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "    stopwords = list(set(stopwords_list.decode().splitlines()))\n",
    "\n",
    "    afinn = requests.get('https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-en-165.txt').content.decode().splitlines()\n",
    "    afinn_dict = dict(map(lambda x: (x.split('\\t')), afinn))\n",
    "    \n",
    "    if type(text) != str:\n",
    "        text = text.decode()\n",
    "    return calc_valence(clean_text(stopwords, text), afinn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part4_mapper_tests.py\n",
    "\n",
    "Testing a large variety of edge cases using the mapper form of the function, each described in function comment. All tests were passed in the .py file.\n",
    "\n",
    "I repeated these tests for the form of the function that will be tested by the grader (where valence()'s only input is the text), and also passed all tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from Part4_mapper import valence, get_afinn_dict\n",
    "import dis\n",
    "import requests\n",
    "\n",
    "class TestValence(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.afinn_dict = get_afinn_dict()\n",
    "        stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "        self.stopwords = list(set(stopwords_list.splitlines()))\n",
    "\n",
    "    def test_normal(self):\n",
    "        '''\n",
    "        A typical sequence of three words\n",
    "        '''\n",
    "        self.assertEqual(valence('yeah winner worst', self.afinn_dict, self.stopwords),[1, 4, -3])\n",
    "\n",
    "    def test_empty(self):\n",
    "        '''\n",
    "        Empty string input\n",
    "        '''\n",
    "        self.assertEqual(valence('', self.afinn_dict, self.stopwords),[])\n",
    "\n",
    "    def test_nonword(self):\n",
    "        '''\n",
    "        Words not in afinn dictionary should be skipped\n",
    "        '''\n",
    "        self.assertEqual(valence('qqqqqq', self.afinn_dict, self.stopwords),[])\n",
    "\n",
    "    def test_quotes(self):\n",
    "        '''\n",
    "        Words in quotes should still parse correctly\n",
    "        '''\n",
    "        self.assertEqual(valence('\"yeah\" \"winner worst\"', self.afinn_dict, self.stopwords),[1, 4, -3])\n",
    "\n",
    "    def test_seperators(self):\n",
    "        '''\n",
    "        Testing that various seperators are removed, and special characters ignored\n",
    "        '''\n",
    "        self.assertEqual(valence('yeah\\twinner\\tworst', self.afinn_dict, self.stopwords),[1, 4, -3])\n",
    "        self.assertEqual(valence('yeah\\t\\twinner\\t\\tworst', self.afinn_dict, self.stopwords),[1, 4, -3])\n",
    "        self.assertEqual(valence('yeah\\nwinner\\nworst\\t\\n', self.afinn_dict, self.stopwords),[1, 4, -3])\n",
    "        self.assertEqual(valence('yeah! *winner[\\n]worst$%^&', self.afinn_dict, self.stopwords),[1, 4, -3])\n",
    "    \n",
    "    def test_nonprintable(self):\n",
    "        '''\n",
    "        Only nonprintable characters are removed\n",
    "        '''\n",
    "        self.assertEqual(valence('\\n', self.afinn_dict, self.stopwords),[])\n",
    "        self.assertEqual(valence('\\n*@$%&($\\n', self.afinn_dict, self.stopwords),[])\n",
    "\n",
    "    def ex_function():\n",
    "        '''\n",
    "        Function to get bytecode of in below test - clean and true both have valences of 2 - no other words in bytecode are present\n",
    "        '''\n",
    "        clean = True\n",
    "    \n",
    "    def test_bytecode_string(self):\n",
    "        '''\n",
    "        Bytecode string should interpret the given instructions\n",
    "        '''\n",
    "        bc_string = dis.Bytecode(self.ex_function).dis()\n",
    "        self.assertEqual(valence(bc_string, self.afinn_dict, self.stopwords),[2, 2])\n",
    "\n",
    "    def test_bytestring(self):\n",
    "        '''\n",
    "        Byte strings should be decoded first\n",
    "        '''\n",
    "        self.assertEqual(valence(b'yeah winner worst', self.afinn_dict, self.stopwords),[1, 4, -3])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part4_mapper.py\n",
    "\n",
    "This valence function is not exactly the same as the form that I changed so it could be used for testing - I refactored the function inputs/outputs slightly so that the stopwords and afinn dictionary did not have to be reloaded on every call to valence(), which happends on every line in the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_stopwords(stopwords, words):\n",
    "    list_ = re.sub(r\"[^a-zA-Z0-9]\", \" \", words.lower()).split()\n",
    "    return [itm for itm in list_ if itm not in stopwords]\n",
    "\n",
    "def clean_text(stopwords, text:str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub(r'[\\d\\n]', ' ', text)\n",
    "    return ' '.join(remove_stopwords(stopwords, text))\n",
    "\n",
    "def get_afinn_dict():\n",
    "    '''\n",
    "    Create a dict from the afinn data, for easier lookup of each word\n",
    "    '''\n",
    "    afinn = requests.get('https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-en-165.txt').content.decode().splitlines()\n",
    "    return dict(map(lambda x: (x.split('\\t')), afinn))\n",
    "\n",
    "def calc_word_valence(word, afinn_dict):\n",
    "    if word in afinn_dict:\n",
    "        return int(afinn_dict[word])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def calc_valence(text, afinn_dict):\n",
    "    '''\n",
    "    Gets the valence of a line of cleaned text, returned as a list of valences at each word\n",
    "    '''\n",
    "    # At this point they will have been cleaned, so we assume a space seperator\n",
    "    word_valences = list(map(lambda word: calc_word_valence(word, afinn_dict), text.split(' ')))\n",
    "    return list(filter(lambda valence: valence is not None, word_valences))\n",
    "\n",
    "def valence(text, afinn_dict, stopwords):\n",
    "    '''\n",
    "    Gets the valence of a line of raw text\n",
    "    '''\n",
    "    # Using afinn_dict and stopwords as inputs so I don't have to load them anew for every line - just once at beginning of mapper\n",
    "    if type(text) != str:\n",
    "        text = text.decode()\n",
    "    return calc_valence(clean_text(stopwords, text), afinn_dict)\n",
    "\n",
    "def main(argv):\n",
    "    stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "    stopwords = list(set(stopwords_list.splitlines()))\n",
    "    afinn_dict = get_afinn_dict()\n",
    "    line = sys.stdin.readline()\n",
    "    filename = Path(os.environ['mapreduce_map_input_file']).stem\n",
    "    pres = filename.split('_')[0]\n",
    "    try:\n",
    "        while line:\n",
    "            valencelist = valence(line, afinn_dict, stopwords)\n",
    "            for v in valencelist: print(pres.title() + \"\\t\" + str(v)) \n",
    "            line = sys.stdin.readline()\n",
    "    except EOFError as error:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part4_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"reducer.py\"\"\"\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_pres = None\n",
    "current_pres_count = 0\n",
    "current_pres_valence_sum = 0\n",
    "pres = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    pres, valence = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        valence = int(valence)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_pres == pres:\n",
    "        current_pres_count += 1\n",
    "        current_pres_valence_sum += valence\n",
    "    else:\n",
    "        if current_pres:\n",
    "            avg_valence = current_pres_valence_sum/current_pres_count\n",
    "            # write result to STDOUT\n",
    "            print ('%s\\t%s' % (current_pres, avg_valence))\n",
    "        current_pres_count = 1\n",
    "        current_pres_valence_sum = valence\n",
    "        current_pres = pres\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "# Avoid divide by zero error\n",
    "if current_pres == pres and current_pres_count != 0:\n",
    "    avg_valence = current_pres_valence_sum/current_pres_count\n",
    "    print ('%s\\t%s' % (current_pres, avg_valence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 - Hadoop Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the modified mapper_noll, all 13 Map task attempts that ran failed, across 5 unique tasks (several had retries that also failed). \n",
    "\n",
    "![P5_1](img/P5_1.png)\n",
    "\n",
    "2 more that were in progress received a kill command, which appears to trigger after one task fails four times - in this case it was task_1728644793739_0003_m_000002\n",
    "\n",
    "![P5_2](img/P5_2.png)\n",
    "\n",
    "When navigating to the logs of each of these 13 failed tasks, the log files all show the same error printed to stderr, the divide by zero error, 13 total error messages. Here is one example:\n",
    "\n",
    "![P5_3](img/P5_3.png)\n",
    "\n",
    "These failed tasks were spread across both worker nodes: /default-rack/quiz4-cluster-w-0.c.cs119-quiz-4.internal:8042 and /default-rack/quiz4-cluster-w-1.c.cs119-quiz-4.internal:8042\n",
    "\n",
    "![P5_4](img/P5_4.png)\n",
    "\n",
    "Comparing this to the logs of the successfull version of this task (From Part 2 Question 5) we can see that each task is charged with analyzing anywhere from 500 to 4,800 lines:\n",
    "\n",
    "![P5_5](img/P5_5.png)\n",
    "\n",
    "We can verify that records refers to lines of text here, since in this example there were 11 tasks and 35,119 total lines of text across the five books - an average of around 3,000 per task is about right. The chance of failure at each line is 1/100, since this is the chance the randomly generated number is 0.\n",
    "Therefore, even the smallest task has only a .99^500=0.65% chance of completing successfully, and the larger tasks are much more unlikely. It is no suprise to see every one of them fail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
