{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CS119 Big Data\n",
        "\n",
        "Spring 2024"
      ],
      "metadata": {
        "id": "wOHI-AYUlyv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "lG600BwylV2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JDK Setup"
      ],
      "metadata": {
        "id": "Tys1Mb_p1H_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing java 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# -q, quiet level 2: no output except for errors\n",
        "#> /dev/null on the end of any command where you want to redirect all the stdout into nothingness\n",
        "#Checking the installed Java version\n",
        "!java -version"
      ],
      "metadata": {
        "id": "rc2-2e2HAnNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f01d73e1-bc22-400f-8faa-f8df0f8584b4"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.24\" 2024-07-16\n",
            "OpenJDK Runtime Environment (build 11.0.24+8-post-Ubuntu-1ubuntu322.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.24+8-post-Ubuntu-1ubuntu322.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Spark\n"
      ],
      "metadata": {
        "id": "HTrf6N9o3079"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "WP-eW9D73xzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53cb70cb-7ee9-4a26-ee7f-4a6b92a078ee"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "I_qmDV9T4K7s"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import functions as f"
      ],
      "metadata": {
        "id": "PRnpatArQZ6j"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "MNc5HlDD032N"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part I - Accumulators"
      ],
      "metadata": {
        "id": "bNc5soHzRrQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Incorrect Code"
      ],
      "metadata": {
        "id": "PM8r-Yh3lb0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [1,2,3,4,5]\n",
        "counter = 0\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Wrong: Don't do this!!\n",
        "def increment_counter(x):\n",
        "    global counter\n",
        "    counter += x\n",
        "\n",
        "rdd.foreach(increment_counter)\n",
        "\n",
        "print(\"Counter value: \", counter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDTPKLK6YxvE",
        "outputId": "7503635f-dbe1-4170-beee-7e288e92932f"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter value:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the incorrect version, just the variable \"counter\" is used, which causes the behavior to be undefined. Each executor only sees a copy of the original counter, 0, and updates their individual copy without being able to see the copy on the driver. Thus, no updates happen, and 0 is returned."
      ],
      "metadata": {
        "id": "mkeHpt0NY7rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Corrected Code:"
      ],
      "metadata": {
        "id": "JKPIDI6SUxtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [1,2,3,4,5]\n",
        "accum = sc.accumulator(0)\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "def increment_counter(x):\n",
        "    accum.add(x)\n",
        "\n",
        "rdd.foreach(increment_counter)\n",
        "\n",
        "print(\"Counter value: \", accum.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT9b_9JERrAx",
        "outputId": "1a092959-30d5-4ea2-f2f0-bef779d02fe9"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter value:  15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This version uses an accumulator, rather than a simple variable. The accumulator is supported by spark, and since it only supports being added to, it can be done efficiently in parallel. They are designed to allow safe updates to a variable in a distrributed environment, resulting in expected and consistent behaviour."
      ],
      "metadata": {
        "id": "--UhjpRmXPgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part II - Airline Traffic"
      ],
      "metadata": {
        "id": "BfEzhmVTZuKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1: Load and Clean Data"
      ],
      "metadata": {
        "id": "hI1yS-nTGiAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the data from the website directly using curl and unzip shell commands"
      ],
      "metadata": {
        "id": "Ym1EH_LOZYA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "june_url = r\"https://www.bts.gov/sites/bts.dot.gov/files/docs/legacy/additional-attachment-files/ONTIME.TD.202406.REL01.06AUG2024.zip\"\n",
        "july_url = r\"https://www.bts.gov/sites/bts.dot.gov/files/docs/legacy/additional-attachment-files/ONTIME.TD.202407.REL01.03SEP2024.zip\"\n",
        "\n",
        "!curl {june_url} > june_data.zip\n",
        "!curl {july_url} > july_data.zip\n",
        "\n",
        "!unzip -p july_data.zip > july_data.csv\n",
        "!unzip -p june_data.zip > june_data.csv\n",
        "\n",
        "# No longer need original archives\n",
        "!rm june_data.zip july_data.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YWyEPQHgd-z",
        "outputId": "42a0b99d-c115-423f-af22-cd96ca994144"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 21.8M  100 21.8M    0     0  36.1M      0 --:--:-- --:--:-- --:--:-- 36.1M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 23.5M  100 23.5M    0     0  80.3M      0 --:--:-- --:--:-- --:--:-- 80.3M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read dataframes from csv files"
      ],
      "metadata": {
        "id": "8Ju9fdvjZfQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "june_df = spark.read.csv('june_data.csv', sep=\"|\", header=None)\n",
        "july_df = spark.read.csv('july_data.csv', sep=\"|\", header=None)\n",
        "full_df = june_df.union(july_df)\n"
      ],
      "metadata": {
        "id": "sGfvB4NncwwX"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change column names to match provided schema"
      ],
      "metadata": {
        "id": "0tw_8jRQZjby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All columns that we know what the schema is are labelled - others are unknown\n",
        "column_names = [\"Carrier\",\"Flight_Number\", \\\n",
        "                \"Unknown1\",\"Unknown2\",\"Unknown3\",\"Unknown4\", \\\n",
        "                \"Departure_Airport\", \\\n",
        "                \"Arrival_Airport\", \\\n",
        "                \"Date_of_Flight_Operation_(Year/Month/Day)\", \\\n",
        "                \"Day_of_Week_of_this_Flight_Operation_(Monday_=_1)\", \\\n",
        "                \"Scheduled_Departure_Time_as_Shown_in_OAG\", \\\n",
        "                \"Scheduled_Departure_Time_as_Shown_in_CRS\", \\\n",
        "                \"Gate_Departure_Time_(Actual)_in_Local_Time\",\"Scheduled_Arrival_Time_as_Shown_in_the_OAG\", \\\n",
        "                \"Scheduled_Arrival_Time_as_Shown_in_CRS\",\"Gate_Arrival_Time_(Actual)_in_Local_Time\", \\\n",
        "                \"Difference_in_Minutes_Between_OAG_and_Scheduled_Departure_Time\", \\\n",
        "                \"Difference_in_Minutes_Between_OAG_and_Scheduled_Arrival_Time\", \\\n",
        "                \"Scheduled_Elapsed_Time_Per_CRS_in_Minutes\", \\\n",
        "                \"Actual_Gate_to_Gate_Time_in_Minutes\", \\\n",
        "                \"Departure_Delay\", \\\n",
        "                \"Arrival_Delay\", \\\n",
        "                \"Elapsed_Time_Difference\", \\\n",
        "                \"Wheels-Off_Time_(Actual)_in_Local_Time\", \\\n",
        "                \"Wheels-On_Time_(Actual)_in_Local_Time\", \\\n",
        "                \"Aircraft_Tail_Number\", \\\n",
        "                \"Unknown5\",\"Unknown6\",\"Unknown7\", \\\n",
        "                \"Cancellation Code\", \\\n",
        "                \"Minutes_late_for_delay_code_E\", \\\n",
        "                \"Minutes_late_for_delay_code_F\", \\\n",
        "                \"Minutes_late_for_delay_code_G\", \\\n",
        "                \"Minutes_late_for_delay_code_H\", \\\n",
        "                \"Minutes_late_for_delay_code_I\"]\n",
        "column_map = {}\n",
        "for i, column in enumerate(full_df.columns):\n",
        "  if i >= len(column_names):\n",
        "    break\n",
        "  column_map[column] = column_names[i]\n",
        "full_df = full_df.withColumnsRenamed(column_map)\n"
      ],
      "metadata": {
        "id": "m82T40-xHCIl"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop columns where we do not know the schema/what the columns represent"
      ],
      "metadata": {
        "id": "bfcHAkpVZnBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop all columns not in schema - either marked with unknown, or never renamed\n",
        "to_drop = []\n",
        "for column in full_df.columns:\n",
        "  if column.startswith('_c') or column.startswith('Unknown'):\n",
        "    to_drop.append(column)\n",
        "\n",
        "full_df = full_df.drop(*to_drop)"
      ],
      "metadata": {
        "id": "XfdXe31yZIAf"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load library of mappings from 2-Letter Airline IATA Codes to Airline names\n"
      ],
      "metadata": {
        "id": "03XL3qE3J4GH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Public online source for mapping of carrier codes to airline names\n",
        "!curl https://gist.githubusercontent.com/AndreiCalazans/390e82a1c3edff852137cb3da813eceb/raw/1a1248f966b3f644f4eae057ad9b9b1b571c6aec/airlines.json > airline_codes.json\n",
        "# Remove the second line from the file to clean - it is a duplicate\n",
        "!sed '2d' airline_codes.json > airline_codes_cleaned.json\n",
        "\n",
        "# Transpose horizontal dataframe to vertical by transposing, taking advantage of Pandas .T - have to convert to pandas and back\n",
        "# Needs to be vertical for joins\n",
        "# This is a relatively small dataframe, so transpose is not too expensive\n",
        "airline_codes_df = spark.read.json(\"airline_codes_cleaned.json\", multiLine=True) \\\n",
        "  .withColumn('IATA', lit('Airline')) \\\n",
        "  .to_pandas_on_spark() \\\n",
        "  .set_index('IATA') \\\n",
        "  .T \\\n",
        "  .reset_index() \\\n",
        "  .rename(columns={\"index\":\"IATA\"}) \\\n",
        "  .to_spark() \\\n",
        "  .withColumnsRenamed({\"IATA\":\"Airline_Code\",\"Airline\":\"Airline_Name\"})\n",
        "\n",
        "\n",
        "airline_codes_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOsg0v0dcRIt",
        "outputId": "d9aebfc5-6aac-4b3c-cb27-ae018b194dc2"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 26040  100 26040    0     0  73719      0 --:--:-- --:--:-- --:--:-- 73559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:5725: FutureWarning: DataFrame.to_pandas_on_spark is deprecated. Use DataFrame.pandas_api instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------------------+\n",
            "|Airline_Code|        Airline_Name|\n",
            "+------------+--------------------+\n",
            "|          0A|           Amber Air|\n",
            "|          0B|            Blue Air|\n",
            "|          0C|        IBL Aviation|\n",
            "|          0D|      Darwin Airline|\n",
            "|          0J|             Jetclub|\n",
            "|          0V|Vietnam Air Servi...|\n",
            "|          1A|    Amadeus IT Group|\n",
            "|          1B|Abacus International|\n",
            "|          1C|Electronic Data S...|\n",
            "|          1D|Radixx Solutions ...|\n",
            "|          1E|Travelsky Technology|\n",
            "|          1F|INFINI Travel Inf...|\n",
            "|          1G|Galileo Internati...|\n",
            "|          1H|        Siren-Travel|\n",
            "|          1I|Sky Trek Internat...|\n",
            "|          1K|               Sutra|\n",
            "|          1L|Open Skies Consul...|\n",
            "|          1M|JSC Transport Aut...|\n",
            "|          1N|           Navitaire|\n",
            "|          1P|           Worldspan|\n",
            "+------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load library of mappings from 3-Letter Airport IATA Codes to Airport names\n"
      ],
      "metadata": {
        "id": "V8w45itLWXlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Public online source for mapping of airport codes to airport names\n",
        "!curl https://raw.githubusercontent.com/datasets/airport-codes/refs/heads/main/data/airport-codes.csv > airport_codes.csv\n",
        "\n",
        "airport_codes_df = spark.read.csv('airport_codes.csv', header=True) \\\n",
        "  .select('name', 'iata_code') \\\n",
        "  .where(col(\"iata_code\").isNotNull()) \\\n",
        "  .withColumnsRenamed({\"name\": \"Airport_Name\", \"iata_code\": \"Airport_Code\"})\n",
        "\n",
        "airport_codes_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAPrRcctOBc8",
        "outputId": "88914802-7e35-4241-c893-3fd4722399af"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 8058k  100 8058k    0     0  32.0M      0 --:--:-- --:--:-- --:--:-- 32.1M\n",
            "+--------------------+------------+\n",
            "|        Airport_Name|Airport_Code|\n",
            "+--------------------+------------+\n",
            "|      Utirik Airport|         UTK|\n",
            "|Ocean Reef Club A...|         OCA|\n",
            "|       Cuddihy Field|         CUX|\n",
            "|Crested Butte Air...|         CSE|\n",
            "|    Columbus Airport|         CUS|\n",
            "|   LBJ Ranch Airport|         JCY|\n",
            "|Loring Seaplane Base|         WLR|\n",
            "| Nunapitchuk Airport|         NUP|\n",
            "|Port Alice Seapla...|         PTC|\n",
            "|     Icy Bay Airport|         ICY|\n",
            "|Port Protection S...|         PPV|\n",
            "|Kalakaket Creek A...|         KKK|\n",
            "|Dunsmuir Muni-Mot...|         MHS|\n",
            "|Chase Field Indus...|         NIR|\n",
            "|Grand Canyon Bar ...|         GCT|\n",
            "|Ellamar Seaplane ...|         ELW|\n",
            "|Lime Village Airport|         LVD|\n",
            "|   Hog River Airport|         HGZ|\n",
            "|      Ed-Air Airport|         OTN|\n",
            "|      Telida Airport|         TLF|\n",
            "+--------------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2: What US Airline Has the Least Delays"
      ],
      "metadata": {
        "id": "UNnXvqWnayEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Consider \"delay\" to be the average of arrival and departure delays\n",
        "q2_df = full_df \\\n",
        "  .withColumn('Mean_Delay', (full_df.Departure_Delay + full_df.Arrival_Delay)/2) \\\n",
        "  .groupBy(\"Carrier\").avg(\"Mean_Delay\") \\\n",
        "  .join(airline_codes_df,full_df.Carrier == airline_codes_df.Airline_Code, \"inner\") \\\n",
        "  .withColumn(\"Average_Delay_Minutes\", f.round(\"avg(Mean_Delay)\",2)) \\\n",
        "  .drop('Airline_Code', 'Carrier', 'avg(Mean_Delay)') \\\n",
        "  .sort(col(\"Average_Delay_Minutes\").asc()) \\\n",
        "  .show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTywXHq57ABx",
        "outputId": "826b84c7-0094-469f-c46f-b8dfd0ab6a75"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+---------------------+\n",
            "|Airline_Name         |Average_Delay_Minutes|\n",
            "+---------------------+---------------------+\n",
            "|Hawaiian Airlines    |5.33                 |\n",
            "|Alaska Airlines, Inc.|7.2                  |\n",
            "|Southwest Airlines   |14.09                |\n",
            "|United Airlines      |15.95                |\n",
            "|Delta Air Lines      |16.12                |\n",
            "|Allegiant Air        |17.96                |\n",
            "|Spirit Airlines      |19.38                |\n",
            "|JetBlue Airways      |20.17                |\n",
            "|American Airlines    |22.06                |\n",
            "|Frontier Airlines    |23.66                |\n",
            "+---------------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hawaiian Airlines has the lowest average delay\n"
      ],
      "metadata": {
        "id": "EBfLNhx7RNNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3: What Departure Time of Day is Best to Avoid\n"
      ],
      "metadata": {
        "id": "3gB3H9EhK1du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Again using delay as the average of arrival and departure\n",
        "q3_df = full_df \\\n",
        "  .withColumn('Time_of_Day',\n",
        "              f.when(col('Scheduled_Departure_Time_as_Shown_in_OAG').between(600, 959),'morning(6AM-10AM)') \\\n",
        "              .when(col('Scheduled_Departure_Time_as_Shown_in_OAG').between(1000, 1359),'midday(10AM-2PM)') \\\n",
        "              .when(col('Scheduled_Departure_Time_as_Shown_in_OAG').between(1400, 1759),'afternoon(2PM-6PM)') \\\n",
        "              .when(col('Scheduled_Departure_Time_as_Shown_in_OAG').between(1800, 2159),'evening(6PM-10PM)') \\\n",
        "              .otherwise('night(10PM-6AM)')\n",
        "              ) \\\n",
        "  .withColumn('Mean_Delay', (full_df.Departure_Delay + full_df.Arrival_Delay)/2) \\\n",
        "  .groupBy(\"Time_of_Day\").avg(\"Mean_Delay\") \\\n",
        "  .withColumn(\"Average_Delay_Minutes\", f.round(\"avg(Mean_Delay)\",2)) \\\n",
        "  .drop('avg(Mean_Delay)') \\\n",
        "  .sort(col(\"Average_Delay_Minutes\").asc()) \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFq8lNtsSipE",
        "outputId": "90aa9a94-2bd0-48a3-bd8c-f4bedbcff03b"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+---------------------+\n",
            "|Time_of_Day       |Average_Delay_Minutes|\n",
            "+------------------+---------------------+\n",
            "|morning(6AM-10AM) |5.98                 |\n",
            "|midday(10AM-2PM)  |12.41                |\n",
            "|night(10PM-6AM)   |16.33                |\n",
            "|afternoon(2PM-6PM)|23.45                |\n",
            "|evening(6PM-10PM) |29.93                |\n",
            "+------------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Morning (6AM-10AM) has the lowest average delay"
      ],
      "metadata": {
        "id": "-U_xFiTaWDZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4: What Airports Have the Most Flight Delays\n"
      ],
      "metadata": {
        "id": "47gFT97bWmIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Need to seperately track departure delays when a given airport was the departure airport, and arrival delays when a given airport was the arrival airport\n",
        "q4_departure_delays_df = full_df \\\n",
        "  .withColumn('Departure_Delay_int', full_df['Departure_Delay'].cast(IntegerType())) \\\n",
        "  .groupBy(\"Departure_Airport\").avg(\"Departure_Delay_int\") \\\n",
        "  .withColumnsRenamed({\"avg(Departure_Delay_int)\":\"Average_Departure_Delay\"}) \\\n",
        "\n",
        "q4_arrival_delays_df = full_df \\\n",
        "  .withColumn('Arrival_Delay_int', full_df['Arrival_Delay'].cast(IntegerType())) \\\n",
        "  .groupBy(\"Arrival_Airport\").avg(\"Arrival_Delay_int\") \\\n",
        "  .withColumnsRenamed({\"avg(Arrival_Delay_int)\":\"Average_Arrival_Delay\"}) \\\n",
        "\n",
        "# Now consider \"delay\" to be the SUM of arrival and departure delays\n",
        "q4_df = q4_departure_delays_df \\\n",
        "  .join(q4_arrival_delays_df, q4_departure_delays_df.Departure_Airport == q4_arrival_delays_df.Arrival_Airport) \\\n",
        "  .withColumn('Average_Total_Delay', q4_departure_delays_df.Average_Departure_Delay + q4_arrival_delays_df.Average_Arrival_Delay) \\\n",
        "  .withColumn('Average_Total_Delay_Minutes', f.round(\"Average_Total_Delay\",2)) \\\n",
        "  .join(airport_codes_df, q4_departure_delays_df.Departure_Airport == airport_codes_df.Airport_Code) \\\n",
        "  .drop('Arrival_Airport', 'Departure_Airport', 'Average_Departure_Delay', 'Average_Arrival_Delay', 'Average_Total_Delay', 'Airport_Code') \\\n",
        "  .sort(col(\"Average_Total_Delay_Minutes\").desc()) \\\n",
        "  .show(n=20, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDl2r77rW3Nn",
        "outputId": "b68e414b-c79b-44fa-d7a5-d26bd33be3d7"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------+-------------------------------------------------------+\n",
            "|Average_Total_Delay_Minutes|Airport_Name                                           |\n",
            "+---------------------------+-------------------------------------------------------+\n",
            "|85.43                      |Hagerstown Regional Richard A Henson Field             |\n",
            "|83.2                       |Lea County Regional Airport                            |\n",
            "|81.21                      |Elko Regional Airport                                  |\n",
            "|78.11                      |Mason City Municipal Airport                           |\n",
            "|75.19                      |Sioux Gateway Airport / Brigadier General Bud Day Field|\n",
            "|74.75                      |Concord-Padgett Regional Airport                       |\n",
            "|72.07                      |Bemidji Regional Airport                               |\n",
            "|71.52                      |Eastern Sierra Regional Airport                        |\n",
            "|68.39                      |Stockton Metropolitan Airport                          |\n",
            "|66.06                      |Henry E Rohlsen Airport                                |\n",
            "|62.03                      |Roswell Air Center Airport                             |\n",
            "|61.38                      |Yeager Airport                                         |\n",
            "|59.55                      |Texarkana Regional Airport (Webb Field)                |\n",
            "|58.54                      |Elmira Corning Regional Airport                        |\n",
            "|58.5                       |Pago Pago International Airport                        |\n",
            "|57.44                      |Dothan Regional Airport                                |\n",
            "|57.2                       |Fort Dodge Regional Airport                            |\n",
            "|55.53                      |Rafael Hernández International Airport                 |\n",
            "|54.69                      |Provo-Utah Lake International Airport                  |\n",
            "|54.48                      |La Crosse Regional Airport                             |\n",
            "+---------------------------+-------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hagerstown Regional Richard A Henson Field had the highest total delay average"
      ],
      "metadata": {
        "id": "M_PvDUoMdhgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 5: What Airports are the Top 5 Busiest Airports in the US"
      ],
      "metadata": {
        "id": "b0VZG4tpdptY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperately count departures when a given airport was the departure airport, and arrivals when a given airport was the arrival airport\n",
        "q5_departure_df = full_df \\\n",
        "  .groupBy('Departure_Airport').count() \\\n",
        "  .withColumnsRenamed({\"count\":\"Total_Departures\"})\n",
        "\n",
        "q5_arrival_df = full_df \\\n",
        "  .groupBy('Arrival_Airport').count() \\\n",
        "  .withColumnsRenamed({\"count\":\"Total_Arrivals\"})\n",
        "\n",
        "q5_df = q5_departure_df \\\n",
        "  .join(q5_arrival_df, q5_departure_df.Departure_Airport == q5_arrival_df.Arrival_Airport) \\\n",
        "  .withColumn('Total_Arrivals_And_Departures', q5_departure_df.Total_Departures + q5_arrival_df.Total_Arrivals) \\\n",
        "  .join(airport_codes_df, q5_departure_df.Departure_Airport == airport_codes_df.Airport_Code) \\\n",
        "  .drop('Arrival_Airport', 'Departure_Airport', 'Total_Departures', 'Total_Arrivals', 'Airport_Code') \\\n",
        "  .sort(col(\"Total_Arrivals_And_Departures\").desc()) \\\n",
        "  .show(n=5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vib1zbfNdvRX",
        "outputId": "ba6f05ef-39f6-4e72-b5da-0a06b27895d4"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+------------------------------------------------+\n",
            "|Total_Arrivals_And_Departures|Airport_Name                                    |\n",
            "+-----------------------------+------------------------------------------------+\n",
            "|119128                       |Hartsfield Jackson Atlanta International Airport|\n",
            "|114319                       |Dallas Fort Worth International Airport         |\n",
            "|111962                       |Denver International Airport                    |\n",
            "|111469                       |Chicago O'Hare International Airport            |\n",
            "|87671                        |Charlotte Douglas International Airport         |\n",
            "+-----------------------------+------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part III - ShortStoryJam"
      ],
      "metadata": {
        "id": "5OYQfj1timIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "S9lGn2W1mDop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf big-data-repo/\n",
        "!git clone https://github.com/singhj/big-data-repo.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrQE-V9_i1wd",
        "outputId": "845eb18d-86a1-4967-d589-e212dc8e7e41"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'big-data-repo'...\n",
            "remote: Enumerating objects: 548, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 548 (delta 117), reused 109 (delta 109), pack-reused 423 (from 1)\u001b[K\n",
            "Receiving objects: 100% (548/548), 56.47 MiB | 22.55 MiB/s, done.\n",
            "Resolving deltas: 100% (295/295), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1: Clean Text and Remove Stopwords"
      ],
      "metadata": {
        "id": "aQWZ3i-Ti8i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import string\n",
        "\n",
        "stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
        "stopwords = list(set(stopwords_list.decode().splitlines()))\n",
        "\n",
        "def remove_stopwords(words):\n",
        "  list_ = re.sub(r\"[^a-zA-Z0-9]\", \" \", words.lower()).split()\n",
        "  return [itm for itm in list_ if itm not in stopwords]\n",
        "\n",
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('\\[.*?\\]', '', text)\n",
        "  text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "  text = re.sub('[\\d\\n]', ' ', text)\n",
        "  return ' '.join(remove_stopwords(text))"
      ],
      "metadata": {
        "id": "FaE-MslLjBX3"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create function \"clean_book\" that cleans the book of a provided name and returns the cleaned text as a single string"
      ],
      "metadata": {
        "id": "jy-a5MTnoSdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_book(book_name):\n",
        "  folder_path = 'big-data-repo/text-proc/poe-stories/'\n",
        "  with open(folder_path + book_name) as f:\n",
        "    cleaned = clean_text(f.read())\n",
        "  f.close()\n",
        "  return cleaned\n",
        "\n",
        "# Testing by outputting first 100 characters of cleaned string\n",
        "print(clean_book('A_DESCENT_INTO_THE_MAELSTROM')[:100])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMfF4nDzmp9G",
        "outputId": "99c89356-32c2-4689-da9b-5ca0dcb4c7f3"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ways god nature providence ways models frame commensurate vastness profundity unsearchableness works\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2: Use NLTK to decompose the first story A_DESCENT_INTO_THE_MAELSTROM"
      ],
      "metadata": {
        "id": "mJnKnLBCo48o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKZMLUAzps0R",
        "outputId": "271f1d8a-5249-4df1-daad-4a1bb1e7b419"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = clean_book('A_DESCENT_INTO_THE_MAELSTROM')\n",
        "\n",
        "def get_pos_tuples(text):\n",
        "  sent_text = nltk.sent_tokenize(text) # this gives us a list of sentences\n",
        "  # now loop over each sentence and tokenize it separately\n",
        "  return [nltk.pos_tag(nltk.word_tokenize(sent)) for sent in sent_text][0]\n",
        "\n",
        "# Printing first 5 as a test\n",
        "print(get_pos_tuples(paragraph)[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIiDy_PDo_ak",
        "outputId": "083d2046-543a-41f8-d0f9-68d867298752"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ways', 'NNS'), ('god', 'VBP'), ('nature', 'JJ'), ('providence', 'NN'), ('ways', 'NNS')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3: Tag non-stopwords as parts of speech using Penn POS Tags"
      ],
      "metadata": {
        "id": "ahh58cNS4LB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.data import load\n",
        "nltk.download('tagsets')\n",
        "taginfo = load('help/tagsets/upenn_tagset.pickle')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M59nk6asvBo3",
        "outputId": "bd7443d6-7c22-478e-857f-2de988ae496a"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "def append_if_match_tag(tag_list:list, pos_tuple, tag):\n",
        "  word, pos = pos_tuple\n",
        "  if pos == tag:\n",
        "    return tag_list + [word]\n",
        "  else:\n",
        "    return tag_list\n",
        "\n",
        "def get_tag_words(tag, tagged_tuples:list[tuple]):\n",
        "  # Returns list of words with that tag\n",
        "  return (tag, reduce(lambda tag_list, pos_tuple: append_if_match_tag(tag_list, pos_tuple, tag), tagged_tuples, []))\n",
        "\n",
        "def get_tag_dict(text, taginfo):\n",
        "  '''\n",
        "  Gets the full tag dictionary of a given piece of clean text\n",
        "  '''\n",
        "  possible_tags = taginfo.keys()\n",
        "  pos_tuples = get_pos_tuples(text)\n",
        "  return dict(list(map(lambda tag: get_tag_words(tag, pos_tuples), possible_tags)))\n",
        "\n",
        "tag_dict = get_tag_dict(paragraph, taginfo)\n",
        "\n",
        "# Printing entire dict as per assignment spec\n",
        "print(tag_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEGhxV8NqLyw",
        "outputId": "a4d09367-eda9-4f85-8089-2da362161ef9"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'LS': [], 'TO': [], 'VBN': ['endured', 'beheld', 'reared', 'called', 'ascended', 'set', 'acquired', 'lashed', 'phrensied', 'assumed', 'round', 'heard', 'received', 'gazed', 'confessed', 'agreed', 'rigged', 'set', 'thought', 'driven', 'drifted', 'rendered', 'weighed', 'set', 'headed', 'batten', 'buried', 'escaped', 'held', 'doomed', 'blazed', 'trimmed', 'recognised', 'possessed', 'headed', 'condemned', 'careered', 'approached', 'secured', 'expected', 'ceased', 'confused', 'revolved', 'met', 'broken', 'floated', 'deceived', 'set', 'called', 'absorbed', 'drawn', 'absorbed', 'explained', 'moved', 'cut', 'thought', 'fastened', 'secured', 'precipitated', 'plunged', 'picked'], \"''\": [], 'WP': [], 'UH': [], 'VBG': ['falling', 'shining', 'particularizing', 'beetling', 'howling', 'shrieking', 'blowing', 'offing', 'dashing', 'increasing', 'moaning', 'chopping', 'changing', 'heaving', 'boiling', 'hissing', 'gyrating', 'whirling', 'plunging', 'spreading', 'entering', 'gleaming', 'shining', 'speeding', 'swaying', 'sweltering', 'sending', 'appalling', 'bewildering', 'conveying', 'splitting', 'returning', 'guarding', 'attempting', 'howling', 'smiling', 'coming', 'rising', 'falling', 'penetrating', 'issuing', 'mentioning', 'standing', 'answering', 'coming', 'starving', 'starting', 'fishing', 'dreaming', 'knowing', 'proposing', 'drifting', 'driving', 'describing', 'chopping', 'ascertaining', 'grasping', 'ring', 'keeping', 'coming', 'driving', 'frothing', 'riding', 'falling', 'roaring', 'letting', 'writhing', 'approaching', 'boasting', 'flying', 'floating', 'ring', 'holding', 'ring', 'raving', 'swaying', 'sickening', 'falling', 'hanging', 'gleaming', 'lying', 'observing', 'maintaining', 'tottering', 'clashing', 'dizzying', 'building', 'speculating', 'exciting', 'entering', 'undergoing', 'floating', 'swimming', 'startling', 'enforcing', 'rendering', 'floating', 'ring', 'quitting', 'bearing', 'setting'], 'JJ': ['nature', 'commensurate', 'unsearchableness', 'speak', 'mortal', 'suppose', 'single', 'jetty', 'black', 'white', 'unstring', 'tremble', 'cliff', 'thrown', 'extreme', 'unobstructed', 'black', 'sixteen', 'half', 'perilous', 'vain', 'danger', 'long', 'sufficient', 'brought', 'close', 'norwegian', 'eighth', 'degree', 'great', 'dreary', 'giddy', 'wide', 'ocean', 'inky', 'nubian', 'desolate', 'human', 'outstretched', 'black', 'cliff', 'high', 'white', 'opposite', 'visible', 'small', 'discernible', 'arose', 'craggy', 'ocean', 'unusual', 'strong', 'landward', 'double', 'hull', 'short', 'angry', 'moskoe', 'ambaaren', 'suarven', 'stockholm', 'true', 'understand', 'hear', 'interior', 'lofoden', 'burst', 'aware', 'loud', 'vast', 'american', 'prairie', 'seamen', 'ocean', 'current', 'gazed', 'current', 'monstrous', 'headlong', 'ungovernable', 'main', 'uproar', 'vast', 'scarred', 'gigantic', 'innumerable', 'eastward', 'precipitous', 'radical', 'general', 'smooth', 'prodigious', 'apparent', 'great', 'form', 'distinct', 'broad', 'terrific', 'smooth', 'black', 'shriek', 'niagara', 'agony', 'threw', 'scant', 'nervous', 'great', 'whirlpool', 'moskoe', 'str', 'ordinary', 'prepared', 'circumstantial', 'wild', 'feeble', 'lofoden', 'afford', 'convenient', 'flood', 'lofoden', 'boisterous', 'impetuous', 'dreadful', 'depth', 'carried', 'thrown', 'ebb', 'weather', 'boisterous', 'dangerous', 'mile', 'impossible', 'fruitless', 'bear', 'swim', 'lofoden', 'stream', 'large', 'current', 'torn', 'consist', 'fro', 'flux', 'high', 'low', 'noise', 'depth', 'shore', 'lofoden', 'sidelong', 'difficult', 'evident', 'attraction', 'phenomenon', 'plausible', 'unsatisfactory', 'flux', 'natural', 'prodigious', 'remote', 'idle', 'hear', 'subject', 'conclusive', 'unintelligible', 'good', 'deaden', 'proceeded', 'vurrgh', 'violent', 'good', 'proper', 'attempt', 'lofoden', 'regular', 'usual', 'great', 'preferred', 'single', 'desperate', 'main', 'str', 'otterholm', 'remain', 'slack', 'steady', 'fail', 'mis', 'stay', 'dead', 'rare', 'arrival', 'boisterous', 'round', 'fouled', 'innumerable', 'lee', 'good', 'twentieth', 'bad', 'good', 'str', 'minute', 'strong', 'current', 'unmanageable', 'eighteen', 'great', 'young', 'horrible', 'tenth', 'blew', 'terrible', 'late', 'steady', 'follow', 'smack', 'fine', 'plenty', 'fresh', 'starboard', 'great', 'unusual', 'feel', 'uneasy', 'astern', 'singular', 'amazing', 'dead', 'long', 'smack', 'attempt', 'experienced', 'feather', 'complete', 'small', 'cross', 'seas', 'foresail', 'flat', 'narrow', 'gunwale', 'bolt', 'mere', 'undoubtedly', 'hold', 'clear', 'rid', 'collect', 'grasp', 'mouth', 'close', 'str', 'violent', 'fit', 'meant', 'wished', 'whirl', 'perceive', 'str', 'whirl', 'hope', 'great', 'fool', 'gun', 'ship', 'seas', 'lay', 'flat', 'absolute', 'singular', 'black', 'circular', 'clear', 'clear', 'deep', 'blue', 'wear', 'lit', 'god', 'light', 'understand', 'hear', 'single', 'shook', 'pale', 'hideous', 'dragged', 'ocean', 'str', 'deep', 'strong', 'gale', 'large', 'strange', 'gigantic', 'sky', 'high', 'sick', 'lofty', 'quick', 'sufficient', 'exact', 'str', 'whirlpool', 'dead', 'str', 'foam', 'sharp', 'larboard', 'shot', 'noise', 'sound', 'imagine', 'whirl', 'thought', 'amazing', 'borne', 'bubble', 'starboard', 'ocean', 'stood', 'huge', 'strange', 'rid', 'great', 'suppose', 'strung', 'reflect', 'magnificent', 'foolish', 'individual', 'wonderful', 'shame', 'depths', 'principal', 'singular', 'general', 'high', 'black', 'mountainous', 'heavy', 'gale', 'great', 'rid', 'petty', 'forbidden', 'uncertain', 'impossible', 'middle', 'horrible', 'stern', 'small', 'coop', 'counter', 'gale', 'large', 'afford', 'madman', 'maniac', 'bolt', 'astern', 'great', 'fro', 'immense', 'lurch', 'headlong', 'hurried', 'sweep', 'open', 'instant', 'lived', 'foam', 'magic', 'interior', 'vast', 'prodigious', 'smooth', 'ebony', 'circular', 'flood', 'golden', 'black', 'inmost', 'general', 'downward', 'unobstructed', 'surface', 'parallel', 'footing', 'dead', 'thick', 'hung', 'narrow', 'great', 'dare', 'foam', 'great', 'descent', 'uniform', 'complete', 'downward', 'perceptible', 'wide', 'liquid', 'visible', 'large', 'timber', 'unnatural', 'original', 'grow', 'dreadful', 'watch', 'strange', 'numerous', 'delirious', 'relative', 'fir', 'tree', 'awful', 'invariable', 'tremble', 'terror', 'great', 'buoyant', 'coast', 'lofoden', 'thrown', 'extraordinary', 'stuck', 'disfigured', 'entered', 'late', 'level', 'ocean', 'general', 'descent', 'equal', 'extent', 'spherical', 'sphere', 'equal', 'cylindrical', 'escape', 'subject', 'forgotten', 'natural', 'bulky', 'great', 'anxious', 'vessel', 'high', 'original', 'resolved', 'loose', 'impossible', 'cask', 'tale', 'bring', 'vast', 'wild', 'rapid', 'foam', 'great', 'vast', 'steep', 'violent', 'gulf', 'uprise', 'clear', 'surface', 'ocean', 'mountainous', 'coast', 'exhausted', 'speechless', 'daily', 'raven', 'black', 'white', 'told', 'merry'], 'VBZ': ['works', 'hairs', 'assumes', 'passages', 'decreases', 'runs', 'leagues', 'whales', 'rocks', 'precipitates', 'rises', 'length', 'boys', 'dark', 'heavens', 'appears', 'surrounds', 'walls', 'ends', 'staves', 'takes', 'masses'], '--': [], 'VBP': ['god', 'frame', 'terror', 'brink', 'mountain', 'guide', 'sea', 'land', 'remote', 'swell', 'island', 'vurrgh', 'sandflesen', 'helseggen', 'sea', 'burst', 'scene', 'foam', 'length', 'vortex', 'ramus', 'beholder', 'calmest', 'roar', 'extent', 'rocks', 'turn', 'disengage', 'firs', 'coast', 'channel', 'whales', 'disappear', 'account', 'ferroe', 'cataract', 'encyclop', 'comprehend', 'creep', 'burthen', 'moskoe', 'sea', 'courage', 'yield', 'slack', 'violent', 'sweeps', 'forget', 'apprehend', 'point', 'puff', 'head', 'boat', 'stupor', 'save', 'wind', 'speak', 'meant', 'flung', 'wave', 'expect', 'turn', 'shriek', 'account', 'gulf', 'shore', 'doubt', 'occupy', 'hold', 'whirl', 'awe', 'mistaken', 'moon', 'abyss', 'moon', 'account', 'pathway', 'funnel', 'mist', 'foam', 'hope', 'rapid', 'whirlpool', 'understand', 'whirl', 'froth', 'lofoden', 'hurricane', 'boat', 'expect', 'lofoden'], 'NN': ['providence', 'vastness', 'profundity', 'democritus', 'joseph', 'glanville', 'summit', 'crag', 'man', 'length', 'route', 'event', 'man', 'man', 'body', 'soul', 'man', 'day', 'change', 'weaken', 'exertion', 'giddy', 'cliff', 'edge', 'rest', 'portion', 'body', 'tenure', 'slippery', 'edge', 'cliff', 'sheer', 'precipice', 'rock', 'dozen', 'truth', 'position', 'companion', 'ground', 'clung', 'shrubs', 'glance', 'sky', 'idea', 'fury', 'reason', 'courage', 'sit', 'distance', 'view', 'scene', 'event', 'story', 'spot', 'eye', 'manner', 'coast', 'latitude', 'province', 'nordland', 'district', 'lofoden', 'mountain', 'sit', 'helseggen', 'cloudy', 'hold', 'grass', 'feel', 'belt', 'vapor', 'beneath', 'sea', 'expanse', 'hue', 'bring', 'mind', 'geographer', 'account', 'mare', 'tenebrarum', 'panorama', 'imagination', 'conceive', 'eye', 'reach', 'character', 'gloom', 'surf', 'promontory', 'apex', 'distance', 'bleak', 'island', 'position', 'wilderness', 'surge', 'size', 'cluster', 'dark', 'appearance', 'space', 'distant', 'island', 'time', 'gale', 'brig', 'lay', 'reefed', 'trysail', 'sight', 'regular', 'quick', 'cross', 'water', 'direction', 'wind', 'foam', 'vicinity', 'distance', 'man', 'mile', 'islesen', 'hotholm', 'buckholm', 'moskoe', 'vurrgh', 'change', 'water', 'caught', 'glimpse', 'sea', 'summit', 'man', 'sound', 'herd', 'moment', 'term', 'character', 'beneath', 'velocity', 'moment', 'speed', 'impetuosity', 'fury', 'moskoe', 'coast', 'bed', 'conflicting', 'convulsion', 'rapidity', 'water', 'alteration', 'surface', 'distance', 'combination', 'gyratory', 'motion', 'germ', 'vast', 'definite', 'existence', 'circle', 'mile', 'diameter', 'edge', 'whirl', 'belt', 'spray', 'particle', 'mouth', 'funnel', 'interior', 'eye', 'fathom', 'jet', 'wall', 'water', 'horizon', 'angle', 'round', 'motion', 'voice', 'half', 'half', 'roar', 'mighty', 'cataract', 'mountain', 'base', 'rock', 'face', 'clung', 'herbage', 'agitation', 'length', 'man', 'maelstr', 'island', 'moskoe', 'midway', 'impart', 'conception', 'magnificence', 'horror', 'scene', 'sense', 'point', 'view', 'writer', 'question', 'time', 'summit', 'helseggen', 'storm', 'description', 'impression', 'spectacle', 'moskoe', 'depth', 'water', 'thirty', 'vurrgh', 'depth', 'passage', 'vessel', 'risk', 'stream', 'country', 'rapidity', 'ebb', 'sea', 'scarce', 'ship', 'attraction', 'beat', 'water', 'relaxes', 'tranquility', 'flood', 'calm', 'quarter', 'hour', 'violence', 'stream', 'fury', 'reach', 'stream', 'violence', 'moskoe', 'borne', 'pine', 'rise', 'broken', 'degree', 'craggy', 'stream', 'reflux', 'sea', 'water', 'year', 'morning', 'sexagesima', 'sunday', 'impetuosity', 'water', 'vicinity', 'vortex', 'reference', 'moskoe', 'depth', 'centre', 'moskoe', 'str', 'proof', 'fact', 'glance', 'abyss', 'whirl', 'crag', 'helseggen', 'pinnacle', 'phlegethon', 'simplicity', 'jonas', 'ramus', 'belief', 'fact', 'thing', 'ship', 'existence', 'influence', 'resist', 'hurricane', 'remember', 'perusal', 'aspect', 'idea', 'collision', 'reflux', 'ridge', 'water', 'flood', 'fall', 'result', 'whirlpool', 'vortex', 'suction', 'lesser', 'dia', 'britannica', 'imagine', 'centre', 'channel', 'maelstr', 'globe', 'gulf', 'bothnia', 'instance', 'opinion', 'imagination', 'guide', 'view', 'notion', 'inability', 'paper', 'absurd', 'thunder', 'abyss', 'whirl', 'man', 'round', 'crag', 'lee', 'roar', 'water', 'story', 'convince', 'moskoe', 'str', 'smack', 'seventy', 'habit', 'fishing', 'fishing', 'business', 'southward', 'fish', 'risk', 'choice', 'variety', 'abundance', 'day', 'craft', 'scrape', 'week', 'fact', 'matter', 'speculation', 'risk', 'life', 'labor', 'courage', 'capital', 'smack', 'cove', 'coast', 'practice', 'fine', 'weather', 'advantage', 'push', 'channel', 'moskoe', 'pool', 'drop', 'anchorage', 'sandflesen', 'time', 'water', 'expedition', 'wind', 'return', 'seldom', 'calculation', 'point', 'night', 'anchor', 'account', 'calm', 'thing', 'remain', 'week', 'death', 'gale', 'occasion', 'sea', 'spite', 'round', 'anchor', 'cross', 'day', 'luck', 'spot', 'weather', 'shift', 'gauntlet', 'moskoe', 'accident', 'heart', 'mouth', 'slack', 'wind', 'smack', 'brother', 'son', 'assistance', 'risk', 'heart', 'danger', 'danger', 'truth', 'day', 'day', 'hurricane', 'morning', 'afternoon', 'breeze', 'south', 'sun', 'shone', 'seaman', 'foreseen', 'clock', 'fish', 'day', 'str', 'slack', 'water', 'wind', 'quarter', 'time', 'rate', 'danger', 'reason', 'aback', 'breeze', 'helseggen', 'boat', 'wind', 'headway', 'return', 'anchorage', 'horizon', 'copper', 'cloud', 'velocity', 'breeze', 'direction', 'state', 'time', 'minute', 'storm', 'sky', 'spray', 'hurricane', 'seaman', 'thing', 'board', 'brother', 'safety', 'boat', 'thing', 'water', 'flush', 'hatch', 'bow', 'hatch', 'custom', 'str', 'precaution', 'circumstance', 'lay', 'brother', 'destruction', 'opportunity', 'threw', 'deck', 'bow', 'foot', 'fore', 'mast', 'instinct', 'thing', 'time', 'breath', 'clung', 'bolt', 'stand', 'dog', 'water', 'measure', 'arm', 'elder', 'brother', 'heart', 'joy', 'moment', 'joy', 'ear', 'word', 'moskoe', 'moment', 'shook', 'head', 'foot', 'ague', 'word', 'understand', 'wind', 'crossing', 'channel', 'calmest', 'weather', 'wait', 'watch', 'pool', 'hurricane', 'moment', 'dream', 'hope', 'time', 'fury', 'spent', 'feel', 'change', 'direction', 'pitch', 'overhead', 'burst', 'rift', 'sky', 'bright', 'moon', 'lustre', 'thing', 'distinctness', 'scene', 'brother', 'manner', 'din', 'word', 'voice', 'head', 'death', 'thought', 'watch', 'fob', 'face', 'moonlight', 'burst', 'clock', 'time', 'slack', 'whirl', 'fury', 'boat', 'laden', 'slip', 'beneath', 'landsman', 'sea', 'phrase', 'ridden', 'sea', 'bore', 'rise', 'sweep', 'slide', 'plunge', 'feel', 'dizzy', 'mountain', 'dream', 'glance', 'glance', 'position', 'instant', 'moskoe', 'quarter', 'mile', 'day', 'whirl', 'race', 'place', 'horror', 'spasm', 'boat', 'half', 'direction', 'thunderbolt', 'moment', 'water', 'kind', 'shrill', 'waste', 'steam', 'steam', 'belt', 'surf', 'moment', 'plunge', 'abyss', 'velocity', 'boat', 'sink', 'water', 'air', 'surface', 'surge', 'whirl', 'larboard', 'wall', 'horizon', 'mind', 'hope', 'deal', 'terror', 'despair', 'truth', 'thing', 'die', 'manner', 'paltry', 'consideration', 'life', 'view', 'manifestation', 'god', 'power', 'idea', 'mind', 'curiosity', 'whirl', 'sacrifice', 'grief', 'man', 'mind', 'extremity', 'boat', 'pool', 'light', 'circumstance', 'possession', 'cessation', 'reach', 'situation', 'surf', 'bed', 'ocean', 'ridge', 'sea', 'form', 'idea', 'confusion', 'mind', 'spray', 'blind', 'deafen', 'strangle', 'power', 'action', 'reflection', 'measure', 'death', 'prison', 'doom', 'circuit', 'belt', 'round', 'hour', 'surge', 'nearer', 'nearer', 'edge', 'time', 'bolt', 'brother', 'water', 'cask', 'thing', 'deck', 'brink', 'pit', 'agony', 'terror', 'force', 'secure', 'grasp', 'grief', 'attempt', 'sheer', 'care', 'contest', 'point', 'difference', 'cask', 'difficulty', 'smack', 'sweeps', 'position', 'starboard', 'prayer', 'god', 'descent', 'hold', 'barrel', 'destruction', 'death', 'water', 'moment', 'moment', 'sense', 'motion', 'vessel', 'exception', 'courage', 'scene', 'forget', 'horror', 'admiration', 'boat', 'surface', 'funnel', 'circumference', 'depth', 'bewildering', 'rapidity', 'spun', 'radiance', 'shot', 'rift', 'glory', 'observe', 'burst', 'terrific', 'grandeur', 'beheld', 'gaze', 'direction', 'view', 'manner', 'smack', 'hung', 'pool', 'keel', 'deck', 'plane', 'water', 'angle', 'beam', 'difficulty', 'situation', 'level', 'speed', 'search', 'profound', 'gulf', 'mist', 'magnificent', 'rainbow', 'bridge', 'time', 'eternity', 'mist', 'spray', 'doubt', 'yell', 'attempt', 'slide', 'abyss', 'distance', 'slope', 'proportionate', 'round', 'round', 'movement', 'circuit', 'whirl', 'progress', 'revolution', 'waste', 'ebony', 'borne', 'boat', 'object', 'embrace', 'whirl', 'house', 'furniture', 'curiosity', 'place', 'drew', 'nearer', 'nearer', 'doom', 'company', 'amusement', 'time', 'thing', 'plunge', 'wreck', 'dutch', 'merchant', 'ship', 'overtook', 'length', 'making', 'fact', 'fact', 'miscalculation', 'train', 'reflection', 'heart', 'dawn', 'hope', 'memory', 'observation', 'variety', 'matter', 'moskoe', 'str', 'number', 'chafed', 'account', 'difference', 'supposing', 'whirl', 'period', 'reason', 'reach', 'turn', 'flood', 'ebb', 'case', 'instance', 'fate', 'rule', 'shape', 'superiority', 'speed', 'descent', 'size', 'shape', 'cylinder', 'school', 'master', 'district', 'explanation', 'fact', 'consequence', 'vortex', 'resistance', 'suction', 'drawn', 'difficulty', 'body', 'form', 'circumstance', 'turn', 'account', 'revolution', 'barrel', 'yard', 'mast', 'level', 'station', 'lash', 'water', 'cask', 'counter', 'throw', 'water', 'attention', 'power', 'length', 'design', 'case', 'shook', 'head', 'station', 'reach', 'emergency', 'delay', 'bitter', 'struggle', 'fate', 'sea', 'moment', 'hesitation', 'result', 'escape', 'possession', 'mode', 'escape', 'anticipate', 'story', 'conclusion', 'hour', 'smack', 'distance', 'beneath', 'succession', 'chaos', 'barrel', 'sunk', 'half', 'distance', 'gulf', 'spot', 'change', 'place', 'character', 'whirlpool', 'slope', 'rainbow', 'sky', 'view', 'spot', 'pool', 'moskoe', 'hour', 'slack', 'sea', 'str', 'fatigue', 'danger', 'memory', 'horror', 'board', 'traveller', 'spirit', 'land', 'hair', 'day', 'expression', 'countenance', 'story', 'faith'], 'DT': [], 'PRP': [], ':': [], 'WP$': [], 'NNPS': [], 'PRP$': [], 'WDT': [], '(': [], ')': [], '.': [], ',': [], '``': [], '$': [], 'RB': ['depth', 'long', 'ago', 'deadly', 'scarcely', 'carelessly', 'beneath', 'deeply', 'length', 'upward', 'dizzily', 'deplorably', 'horridly', 'forcibly', 'ghastly', 'forever', 'properly', 'nearer', 'hideously', 'constantly', 'midway', 'northward', 'gradually', 'rapidly', 'eastward', 'vurrgh', 'sway', 'suddenly', 'suddenly', 'suddenly', 'dizzily', 'heaven', 'excess', 'exceedingly', 'weather', 'moskoe', 'noise', 'heard', 'inevitably', 'gradually', 'storm', 'norway', 'likewise', 'frequently', 'terribly', 'shore', 'plainly', 'constantly', 'early', 'ground', 'regard', 'close', 'immeasurably', 'deadly', 'bodily', 'generally', 'decidedly', 'universally', 'altogether', 'schooner', 'shortly', 'violently', 'stout', 'afterward', 'brightly', 'suddenly', 'folly', 'norway', 'cleverly', 'mainmast', 'completely', 'presently', 'overboard', 'horror', 'bound', 'long', 'carefully', 'slack', 'ear', 'presently', 'properly', 'cleverly', 'presently', 'ahead', 'involuntarily', 'afterward', 'suddenly', 'subside', 'completely', 'indistinctly', 'positively', 'considerably', 'gradually', 'securely', 'overboard', 'steadily', 'scarcely', 'abyss', 'felt', 'instinctively', 'midway', 'perfectly', 'ghastly', 'accurately', 'instinctively', 'scarcely', 'distinctly', 'nature', 'heavily', 'arose', 'partly', 'partly', 'mind', 'appearance', 'distinctly', 'completely', 'slowly', 'early', 'rapidly', 'slowly', 'sphere', 'equally', 'longer', 'securely', 'despairingly', 'counter', 'precisely', 'brother', 'headlong', 'forever', 'farther', 'overboard', 'momently', 'gradually', 'slowly', 'moon', 'radiantly', 'borne', 'violently', 'scarcely'], 'RBR': ['shadow', 'farther', 'matter', 'feather', 'higher', 'longer', 'listen', 'explore', 'farther', 'limbs', 'cylinder', 'farther'], 'RBS': [], 'VBD': ['reached', 'exhausted', 'guided', 'happened', 'happened', 'survived', 'frightened', 'hung', 'arose', 'tempted', 'excited', 'fell', 'dared', 'struggled', 'mentioned', 'continued', 'distinguished', 'looked', 'wore', 'left', 'lay', 'illustrated', 'enveloped', 'encompassed', 'shore', 'plunged', 'resumed', 'keildhelm', 'thought', 'spoke', 'perceived', 'held', 'seamed', 'grew', 'disappeared', 'subsided', 'represented', 'slipped', 'inclined', 'trembled', 'rocked', 'termed', 'surveyed', 'quoted', 'equalled', 'absorbed', 'heightened', 'carried', 'overpowered', 'caught', 'roared', 'absorbed', 'grew', 'whirled', 'regulated', 'raged', 'fell', 'ascertained', 'appeared', 'wore', 'named', 'assented', 'surprised', 'entertained', 'desired', 'owned', 'weighed', 'felt', 'forced', 'blew', 'threw', 'dragged', 'brought', 'encountered', 'happened', 'thought', 'occurred', 'july', 'crossed', 'loaded', 'remarked', 'started', 'knew', 'spanked', 'happened', 'began', 'covered', 'colored', 'rose', 'fell', 'becalmed', 'blew', 'sawed', 'lashed', 'sat', 'foundered', 'prompted', 'flurried', 'deluged', 'raised', 'felt', 'leaped', 'turned', 'screamed', 'knew', 'knew', 'drove', 'thought', 'cursed', 'knew', 'scudded', 'knew', 'increased', 'screamed', 'held', 'flashed', 'glanced', 'built', 'called', 'happened', 'rose', 'believed', 'moskoe', 'closed', 'clenched', 'felt', 'enveloped', 'drowned', 'wore', 'skim', 'arose', 'left', 'composed', 'unmanned', 'began', 'blushed', 'crossed', 'felt', 'thought', 'rendered', 'tended', 'belt', 'towered', 'occasioned', 'allowed', 'lashed', 'swept', 'endeavored', 'felt', 'knew', 'fright', 'knew', 'held', 'flew', 'rushed', 'muttered', 'thought', 'tightened', 'closed', 'dared', 'wondered', 'elapsed', 'belt', 'lay', 'looked', 'gazed', 'appeared', 'streamed', 'recovered', 'fell', 'inclined', 'lay', 'sloped', 'suppose', 'enveloped', 'occasioned', 'belt', 'carried', 'swept', 'perceived', 'appeared', 'began', 'sought', 'disappointed', 'beat', 'strewed', 'shattered', 'roughened', 'recollected', 'roughened', 'absorbed', 'descended', 'conceived', 'whirled', 'absorbed', 'learned', 'observed', 'happened', 'offered', 'passed', 'opened', 'hesitated', 'held', 'attracted', 'pointed', 'comprehended', 'refused', 'admitted', 'resigned', 'hoped', 'effected', 'descended', 'loved', 'attached', 'leaped', 'grew', 'disappeared', 'str', 'heaved', 'hurried', 'removed', 'drew', 'knew', 'changed'], 'IN': ['broken', 'teeth', 'otterholm', 'ver', 'abyss', 'amid', 'drove', 'overcast', 'thrown', 'wind', 'wind', 'round', 'round', 'wild', 'amid', 'tide', 'thereabout'], 'FW': ['elbow', 'kircher'], 'RP': [], 'JJR': ['greater', 'weightier', 'higher', 'smaller', 'yonder', 'greater', 'smaller', 'higher', 'deeper', 'lower', 'greater', 'counter', 'restore', 'lower', 'deeper', 'smaller', 'greater', 'larger', 'cylinder', 'greater', 'brother'], 'JJS': ['loftiest', 'youngest', 'divest', 'crest', 'faintest', 'loudest', 'highest', 'honest', 'largest', 'finest', 'eldest', 'west', 'oldest', 'worst', 'slightest', 'oldest', 'youngest', 'lightest', 'tempest', 'greatest', 'keenest', 'west'], 'PDT': [], 'MD': [], 'VB': ['raise', 'timid', 'morrow', 'watch', 'deck', 'elder', 'shake', 'slack', 'keel', 'hold', 'slow', 'channel'], 'WRB': [], 'NNP': [], 'EX': [], 'NNS': ['ways', 'ways', 'models', 'minutes', 'sons', 'years', 'hours', 'limbs', 'nerves', 'feet', 'crags', 'yards', 'foundations', 'winds', 'fancies', 'waters', 'ramparts', 'lines', 'miles', 'miles', 'barren', 'intervals', 'rocks', 'rocks', 'norwegians', 'flimen', 'names', 'places', 'minutes', 'buffaloes', 'minutes', 'waters', 'channels', 'vortices', 'descents', 'minutes', 'whirlpools', 'streaks', 'streaks', 'vortices', 'degrees', 'winds', 'lifts', 'norwegians', 'accounts', 'jonas', 'confounds', 'details', 'fathoms', 'rocks', 'cataracts', 'vortices', 'pits', 'pieces', 'fragments', 'intervals', 'boats', 'yachts', 'ships', 'howlings', 'bellowings', 'struggles', 'stocks', 'trees', 'bristles', 'rocks', 'hours', 'stones', 'houses', 'fathoms', 'portions', 'records', 'anecdotes', 'bears', 'attempts', 'vortices', 'islands', 'waves', 'shelves', 'confines', 'experiments', 'norwegians', 'brothers', 'tons', 'islands', 'eddies', 'opportunities', 'coastmen', 'islands', 'grounds', 'hours', 'places', 'spots', 'rocks', 'miles', 'minutes', 'eddies', 'years', 'grounds', 'channel', 'whirlpools', 'currents', 'flimen', 'difficulties', 'grounds', 'times', 'years', 'times', 'days', 'years', 'people', 'heavens', 'gentle', 'brothers', 'islands', 'eddies', 'things', 'sails', 'masts', 'moments', 'feet', 'hands', 'moments', 'knees', 'hands', 'seas', 'senses', 'feelings', 'str', 'times', 'events', 'mountains', 'attempts', 'fingers', 'tears', 'waves', 'swells', 'eyes', 'lids', 'minutes', 'waves', 'pipes', 'vessels', 'jaws', 'felt', 'nerves', 'companions', 'mysteries', 'fancies', 'revolutions', 'annoyances', 'felons', 'indulgences', 'hands', 'swelters', 'eyes', 'seconds', 'struggles', 'sensations', 'sides', 'rays', 'clouds', 'recesses', 'degrees', 'rays', 'mussulmen', 'walls', 'heavens', 'swings', 'jerks', 'yards', 'fragments', 'vessels', 'masses', 'trunks', 'trees', 'articles', 'pieces', 'boxes', 'barrels', 'terrors', 'things', 'velocities', 'descents', 'disappears', 'guesses', 'articles', 'splinters', 'fragments', 'observations', 'bodies', 'masses', 'conversations', 'forms', 'fragments', 'observations', 'things', 'eyes', 'wonders', 'signs', 'barrels', 'bolt', 'lashings', 'gyrations', 'sides', 'funnel', 'gyrations', 'degrees', 'winds', 'shores', 'waves', 'effects', 'minutes', 'grounds', 'fishermen', 'mates', 'companions', 'fishermen'], 'SYM': [], 'CC': [], 'CD': [], 'POS': []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4: Build a dataframe to store the text and title of the story, along with columns representing the two-letter prefixes of each tag"
      ],
      "metadata": {
        "id": "YiprhnWuBYB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
        "from functools import reduce\n",
        "from copy import deepcopy\n",
        "import os\n",
        "\n",
        "def append_if_starts_with(prefix_dict, dict_tuple):\n",
        "  '''\n",
        "  Helper function for reducer that converts the tag dictionary to one where\n",
        "  only certain two-letter prefixes are considered, and all categories with\n",
        "  that prefix are combined\n",
        "  '''\n",
        "  prefix_list=['NN', 'VB', 'JJ', 'RB']\n",
        "  pos, word_list = dict_tuple\n",
        "  new_dict = deepcopy(prefix_dict)\n",
        "  if len(pos) >= 2 and pos[:2] in prefix_list:\n",
        "    prefix = pos[:2]\n",
        "    if prefix in prefix_dict:\n",
        "        old_val = prefix_dict[prefix]\n",
        "    else:\n",
        "        old_val = []\n",
        "    return new_dict | {prefix: old_val + word_list}\n",
        "  else:\n",
        "    return new_dict\n",
        "\n",
        "def convert_to_prefix_dict(tag_dict):\n",
        "    '''\n",
        "    Converts a tag dictionary to a new dicitonary that combines values by\n",
        "    two-letter prefix of the key\n",
        "    '''\n",
        "\n",
        "    return reduce(append_if_starts_with, list(tag_dict.items()), {})\n",
        "\n",
        "\n",
        "def add_book(df, title):\n",
        "  '''\n",
        "  Adds the information of a book to a row of a distributed dataframe\n",
        "  '''\n",
        "  paragraph = clean_book(title)\n",
        "  tag_dict = get_tag_dict(paragraph, taginfo)\n",
        "  prefix_dict = convert_to_prefix_dict(tag_dict)\n",
        "  schema = StructType([StructField(key, ArrayType(StringType()), True) for key in prefix_dict])\n",
        "  result_df = spark.createDataFrame([prefix_dict], schema=schema) \\\n",
        "    .withColumn('Title', lit(title.replace(\"_\",\" \").title())) \\\n",
        "    .withColumn('text', lit(paragraph))\n",
        "  if df is None:\n",
        "    return result_df\n",
        "  else:\n",
        "    return df.union(result_df)\n",
        "\n",
        "# Add all books in folder to dataframe\n",
        "df = None\n",
        "for book in os.listdir('big-data-repo/text-proc/poe-stories/'):\n",
        "  df = add_book(df, book)\n",
        "\n",
        "# This displays better in the ipynb file:\n",
        "df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16z-OBB54nKc",
        "outputId": "cbd58cf9-f241-41e1-a855-dc08c030284e"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                  VB|                  JJ|                  NN|                  RB|               Title|                text|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|[published, hurri...|[elaborate, summa...|[minute, paper, j...|[simply, ago, spe...|Von Kempelen And ...|minute elaborate ...|\n",
            "|[honored, connect...|[dicebant, amicae...|[mihi, visitarem,...|[intimately, last...|            Berenice|dicebant mihi sod...|\n",
            "|[purloined, maint...|[odiosius, seneca...|[letter, nil, sap...|[observer, intent...|The Purloined Letter|purloined letter ...|\n",
            "|[fallen, risen, c...|[fail, vale, exeq...|[stay, meet, deat...|[youth, art, sure...|     The Assignation|stay fail meet th...|\n",
            "|[enjoyed, estimat...|[nullus, enim, mo...|[locus, sine, gen...|[mockery, lui, fu...|The Island Of The...|nullus enim locus...|\n",
            "|[undulated, hit, ...|[arnheim, pedestr...|[pendant, domain,...|[remarkably, conf...|     Landors Cottage|pendant domain ar...|\n",
            "|[selected, endure...|[horrible, legiti...|[fiction, romanti...|[regard, vividly,...|The Premature Burial|themes absorbing ...|\n",
            "|[set, fathomed, b...|[prima, human, ra...|[consideration, s...|[equally, solely,...|The Imp Of The Pe...|consideration fac...|\n",
            "|[concerned, rende...|[pretend, extraor...|[matter, case, va...|[naturally, succi...|The Facts In The ...|pretend matter ex...|\n",
            "|[paused, pondered...|[suspendu, dull, ...|[son, luth, sit, ...|[oppressively, si...|The Fall Of The H...|son luth suspendu...|\n",
            "|[quoted, pardoned...|[oriental, tellme...|[truth, stranger,...|[scarcely, discov...|The Thousand-And-...|truth stranger fi...|\n",
            "|[refined, suppose...|[rationale, mere,...|[doubt, mesmerism...|[universally, mer...| Mesmeric Revelation|doubt envelop rat...|\n",
            "|[exalted, loved, ...|[specific, anima,...|[conservatione, f...|[lully, wisdom, a...|            Eleonora|conservatione for...|\n",
            "|[resolved, contin...|[borne, ventured,...|[insult, soul, su...|[definitively, eq...|The Cask Of Amont...|injuries fortunat...|\n",
            "|[agitated, fallen...|[silent, listen, ...|[mountain, hand, ...|[forever, forever...|     Silence-A Fable|mountain pinnacle...|\n",
            "|[endured, beheld,...|[nature, commensu...|[providence, vast...|[depth, long, ago...|A Descent Into Th...|ways god nature p...|\n",
            "|[resolved, red, e...|[red, devastated,...|[death, country, ...|[long, august, fo...|The Masque Of The...|red death long de...|\n",
            "|[thought, feel, s...|[longos, hic, inn...|[impia, tortorum,...|[long, presently,...|The Pit And The P...|impia tortorum lo...|\n",
            "|[scorn, bruited, ...|[grim, spectre, f...|[conscience, path...|[art, forever, et...|      William Wilson|conscience grim s...|\n",
            "|[tortured, derive...|[wild, narrative,...|[belief, mad, cas...|[homely, surely, ...|       The Black Cat|wild homely narra...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 5: Determine if the number of different parts of speech"
      ],
      "metadata": {
        "id": "d3Oj6bqRdc5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import size\n",
        "\n",
        "p3q5_df = df \\\n",
        "  .withColumn('Total_Words', size(f.split('text', \" \"))) \\\n",
        "  .withColumn('Verbs_per_1000', f.round(1000*size('VB')/col('Total_Words'),0).cast('integer')) \\\n",
        "  .withColumn('Nouns_per_1000', f.round(1000*size('NN')/col('Total_Words'),0).cast('integer')) \\\n",
        "  .withColumn('Adjectives_per_1000', f.round(1000*size('JJ')/col('Total_Words'),0).cast('integer')) \\\n",
        "  .withColumn('Adverbs_per_1000', f.round(1000*size('RB')/col('Total_Words'),0).cast('integer')) \\\n",
        "  .drop('VB', 'NN', 'JJ', 'RB', 'text', 'Total_Words')\n",
        "\n",
        "\n",
        "p3q5_df.show(n=df.count(), truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWkiW6kI_sKF",
        "outputId": "eaa61bc1-8f29-41aa-9141-57d801b25d34"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------+--------------+--------------+-------------------+----------------+\n",
            "|Title                                       |Verbs_per_1000|Nouns_per_1000|Adjectives_per_1000|Adverbs_per_1000|\n",
            "+--------------------------------------------+--------------+--------------+-------------------+----------------+\n",
            "|Von Kempelen And His Discovery              |219           |472           |236                |64              |\n",
            "|Berenice                                    |219           |471           |242                |62              |\n",
            "|The Purloined Letter                        |221           |507           |221                |44              |\n",
            "|The Assignation                             |226           |484           |235                |49              |\n",
            "|The Island Of The Fay                       |218           |459           |251                |61              |\n",
            "|Landors Cottage                             |194           |457           |275                |67              |\n",
            "|The Premature Burial                        |229           |463           |241                |63              |\n",
            "|The Imp Of The Perverse                     |206           |497           |235                |55              |\n",
            "|The Facts In The Case Of M. Valdemar        |219           |466           |243                |69              |\n",
            "|The Fall Of The House Of Usher              |219           |468           |243                |63              |\n",
            "|The Thousand-And-Second Tale Of Scheherazade|222           |495           |224                |51              |\n",
            "|Mesmeric Revelation                         |177           |516           |246                |53              |\n",
            "|Eleonora                                    |236           |475           |224                |59              |\n",
            "|The Cask Of Amontillado                     |222           |473           |240                |59              |\n",
            "|Silence-A Fable                             |230           |479           |215                |67              |\n",
            "|A Descent Into The Maelstrom                |215           |492           |221                |64              |\n",
            "|The Masque Of The Red Death                 |221           |501           |221                |53              |\n",
            "|The Pit And The Pendulum                    |245           |478           |212                |60              |\n",
            "|William Wilson                              |216           |473           |245                |58              |\n",
            "|The Black Cat                               |220           |485           |234                |56              |\n",
            "|The Domain Of Arnheim                       |198           |495           |249                |51              |\n",
            "+--------------------------------------------+--------------+--------------+-------------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import variation\n",
        "\n",
        "verbs_cv = variation([row['Verbs_per_1000'] for row in p3q5_df.collect()])\n",
        "nouns_cv = variation([row['Nouns_per_1000'] for row in p3q5_df.collect()])\n",
        "adjs_cv = variation([row['Adjectives_per_1000'] for row in p3q5_df.collect()])\n",
        "advs_cv = variation([row['Adverbs_per_1000'] for row in p3q5_df.collect()])\n",
        "\n",
        "print(\"Verbs Coefficient of Variation: \" + str(verbs_cv))\n",
        "print(\"Nouns Coefficient of Variation: \" + str(nouns_cv))\n",
        "print(\"Adjectives Coefficient of Variation: \" + str(adjs_cv))\n",
        "print(\"Adverbs Coefficient of Variation: \" + str(advs_cv))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2XDBFDPh0NN",
        "outputId": "f8450f1b-237a-4722-a2dc-b7affc79cabc"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verbs Coefficient of Variation: 0.06536039141700235\n",
            "Nouns Coefficient of Variation: 0.03279240652582868\n",
            "Adjectives Coefficient of Variation: 0.060363064825201665\n",
            "Adverbs Coefficient of Variation: 0.11002228396917048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest coefficient of variation among all 4 parts of speech is only .11, which means its standard deviation is only 11% as large as its mean. I would consider this to be very small, which is consistent with the conjecture that we would expect the frequency of occurence of each part of speech to be approximately the same throughout each book."
      ],
      "metadata": {
        "id": "W07n9Easje1z"
      }
    }
  ]
}